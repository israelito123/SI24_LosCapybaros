{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descenso de gradiente para regresión lineal\n",
    "\n",
    "En el ejercicio anterior aprendiste regresión lineal y encontraste una solución analítica a través de algebra lineal, ahora resolveremos el mismo problema llegando a la solución de manera iterativa. \n",
    "\n",
    "En la práctica, descenso de gradiente se utiliza en problemas en los que obtener una solución analítica no es posible como regresión logística o redes neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2) (100, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x112e815d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Nuevamente generamos los datos aleatorios con una desviación estandar conocida\n",
    "# Alrededor de una funcion lineal conocida y = 1.2x + 0.7\n",
    "n_samples = 300\n",
    "ruido_blanco_std = 2\n",
    "X = np.random.randn(n_samples, 1)\n",
    "for_bias = np.ones([n_samples, 1])\n",
    "X_augmented = np.concatenate([for_bias, X], axis=-1)\n",
    "eps = np.random.randn(n_samples, 1) * ruido_blanco_std\n",
    "ideal_w = [[5], [1.2]]\n",
    "target_y = X_augmented @ ideal_w + eps\n",
    "\n",
    "# Dividir en entrenamiento y validación\n",
    "n_val_dp = n_samples//3\n",
    "x_train = X_augmented[:-n_val_dp,:]\n",
    "y_train = target_y[:-n_val_dp]\n",
    "\n",
    "x_val = X_augmented[-n_val_dp:,:]\n",
    "y_val = target_y[-n_val_dp:]\n",
    "\n",
    "print(x_train.shape, x_val.shape)\n",
    "# Graficar set de entrenamiento\n",
    "plt.scatter(x_train[:,-1], y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de costo\n",
    "Ya que hemos cargado el dataset y sabemos como está compuesto, debemos calcular la función de costo\n",
    "\n",
    "$ MSE = J(w) = \\frac{1}{2n} \\sum_{i=1}^{n} (\\hat{Y}_i - Y_i)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y, y_pred):\n",
    "    \"\"\"\n",
    "        Regresa el error cuadrático promedio de todas las predicciones\n",
    "        y: shape=N Etiquetas\n",
    "        y_pred: predicciones del modelo\n",
    "    \"\"\"\n",
    "    # TODO: Calcula el resultado de la función de costo\n",
    "    # ====== Start of solution =====\n",
    "    loss = 0\n",
    "    for index in range (n_val_dp):\n",
    "        loss+= Math.pow((y_pred-y),2)\n",
    "    return loss*(1/(2*(n_val_dp)))\n",
    "    # ====== End of solution =====\n",
    "\n",
    "# Iniciamos con unos parametros w arbitrarios\n",
    "w = np.random.uniform(-10, 10, size=(2, 1))\n",
    "\n",
    "# TODO: Calcula las preddiciones para el conjunto de entrenamiento x_train\n",
    "y_pred = []\n",
    "\n",
    "for index in range (len(n_val_dp)):\n",
    "    y_pred.append(ideal_w[0]+ideal_w[1]*x_train[index])\n",
    "\n",
    "# TODO: Calcula la función de costo para las predicciones(y_pred) y los valores reales (y_train)\n",
    "# ====== Start of solution =====\n",
    "loss = 0\n",
    "for index in range (n_val_dp):\n",
    "    loss+= Math.pow((w[index]-(ideal_w[0]+ideal_w[1]*x_train[index])),2)\n",
    "\n",
    "loss = loss*(1/(2*(n_val_dp)))\n",
    "print\n",
    "# ====== End of solution ====="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar como se ve la función de costo con diferentes pesos.\n",
    "Si elegimos los pesos que generan el menor costo en el set habremos encontrado los parametros ideales para este problema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probaremos con 100x100 combinaciones de distintos parametros\n",
    "w0 = np.linspace(-10, 10, 100)\n",
    "w1 = np.linspace(-10, 10, 100)\n",
    "w0w0, w1w1 = np.meshgrid(w0, w1)\n",
    "w0_flat = w0w0.ravel()\n",
    "w1_flat = w1w1.ravel()\n",
    "W = np.stack([w0_flat, w1_flat], axis=0)\n",
    "\n",
    "\n",
    "# Calculamos la función de costo\n",
    "# TODO: calcula el costo de las predicciones (y_pred) contra etiquetas (y_train)\n",
    "# usando las 100x100 combindaciones de parametros (W)\n",
    "# ====== Start of solution =====\n",
    "y_pred = ...\n",
    "calc_cost = ...\n",
    "# ====== End of solution =====\n",
    "\n",
    "# Graficamos los parametros w con su respectivo costo en el eje de las x\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(w0_flat, w1_flat, calc_cost)\n",
    "\n",
    "# TODO: Escribe a qué valor corresponde cada eje\n",
    "# ====== Start of solution =====\n",
    "ax.set_xlabel(...)\n",
    "ax.set_ylabel(...)\n",
    "ax.set_zlabel(...)\n",
    "# ====== End of solution =====\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos encontrar la combinación de pesos que minimizan el costo\n",
    "# Y de esta manera encontrar los pesos ideales\n",
    "best_idx = np.argmin(calc_cost)\n",
    "search_w0 = w0_flat[best_idx]\n",
    "search_w1 = w1_flat[best_idx]\n",
    "search_w = [[search_w0], [search_w1]] \n",
    "\n",
    "plt.scatter(X, target_y)\n",
    "# TODO: Grafica la línea de regresión generada por tu modelo search_w\n",
    "# Encima de las muestras dadas\n",
    "# ====== Start of solution =====\n",
    "pred_y = ...\n",
    "plt.plot(...)\n",
    "# ====== End of solution ====="
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el código anterior evaluamos $100 \\times 100$ combinaciones de pesos posibles y encontramos los mejores pesos. En este caso solo tenemos 2 parametros que queremos estimar, por lo que encontrar la solución de esta manera es factible al tener que realizar alrededor de $100^2$ evaluaciones de la función. \n",
    "\n",
    "<b>TODO: Contesta las siguientes preguntas</b> \n",
    "- ¿Cuántas evaluaciones de la función de costo tendríamos que hacer si tuviéramos 4 parametros?\n",
    "- ¿Qué pasaría si tuviéramos 1 millon de parametros?\n",
    "\n",
    "Las redes neuronales tienen millones de parametros para hacer predicciones, por lo que encontrar la solución de esta manera no es factible. Se han encontrado formas más eficientes de encontrar la solución por ejemplo, a través de métodos iterativos. Uno de ellos es decenso de gradiente. Siguiendo la direccion contraria del gradiente del costo con respecto a los pesos, podemos encontrar los parámetros que <b>minimizan</b> la función de costo utilizando menos evaluaciones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descenso de gradiente\n",
    "En esta sección, optimizarás la función de regresión lineal a través de descenso de gradiente.\n",
    "La predicción de nuestro modelo está dada con la siguiente funcion:\n",
    "\n",
    "$ \\hat{Y}_i = w_0 + w_1 * x$\n",
    "\n",
    "$\\hat{Y} = \\mathbf{X}\\mathbf{w}$\n",
    "\n",
    "<!-- El gradiente de MSE con respecto a $w_0$ se calcula de la siguiente manera\n",
    "\n",
    "$ \\frac{\\partial J(w)}{\\partial w_0} = \\frac{\\partial J(w)}{\\partial \\hat{Y}_i} * \\frac{\\partial \\hat{Y}_i}{\\partial w_0}= \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{Y}_i - Y_i) $\n",
    "\n",
    "El gradiente de MSE con respecto a $w_1$ se calcula de la siguiente manera\n",
    "\n",
    "$ \\frac{\\partial J(w)}{\\partial w_1} = \\frac{\\partial J(w)}{\\partial \\hat{Y}_i} * \\frac{\\partial \\hat{Y}_i}{\\partial w_1} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{Y}_i - Y_i) * x_i $ -->\n",
    "\n",
    "Donde:\n",
    "- $\\hat{Y}_i \\in  \\hat{Y}$\n",
    "- $\\mathbf{Y} \\in \\mathbb{R}^{N}$\n",
    "- $\\mathbf{X} \\in  \\mathbb{R}^{N \\times D}$ en notación aumentada\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^{D}$ en notación aumentada\n",
    "\n",
    "Como vimos en clase, el gradiente la función de costo con respecto a los pesos está dado por:\n",
    "\n",
    " $\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} = -\\frac{1}{N}X^T\\epsilon$\n",
    "\n",
    " donde $\\epsilon=\\mathbf{Y}-\\mathbf{X}\\mathbf{w}$\n",
    "\n",
    " TODO: Completa el código necesario para aplicar descenso de gradiente durante n_iteraciones. Para ello realiza los siguientes pasos:\n",
    "- Calcula el valor de las predicciones $\\mathbf{Y}$\n",
    "- Calcula el gradiente de la función a optimizar $\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}}$\n",
    "- Actualiza los pesos con el gradiente $\\mathbf{w}^{i+1} = \\mathbf{w}^{i} - \\alpha \\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}}$\n",
    "- Repite hasta optener el valor óptimo de $\\mathbf{w}$\n",
    "\n",
    "Es tiempo de programarlo en código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, lr, N_iterations, w_start=np.array([0,0])):\n",
    "    \"\"\"\n",
    "    Entradas:\n",
    "    X: arreglo con los datos en notación aumentada X\n",
    "    y: vector de etiquetas\n",
    "    lr: ritmo de aprendizaje\n",
    "    N_iterations: cantidad de iteraciones de optimización\n",
    "    w_start: pesos iniciales\n",
    "        \n",
    "    Regresa:  \n",
    "    w_opt: pesos óptimos\n",
    "    cost_history: arreglo con el valor del costo para cada iteración\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # initialize the coefficients\n",
    "    w=w_start\n",
    "    \n",
    "    for i in range(N_iterations):\n",
    "        # ====== Start of solution =====\n",
    "        # TODO: Calcula la predicción \n",
    "        hypothesis = ...\n",
    "\n",
    "        # TODO: Calcula el error\n",
    "\n",
    "\n",
    "        # TODO: Calcula el gradiente del MSE loss\n",
    "\n",
    "\n",
    "        # TODO: Actualiza los pesos\n",
    "\n",
    "        \n",
    "        # ====== End of solution =====\n",
    "        # Calculamos la función de costo y guardamos el resultado\n",
    "        loss = mse_loss(y, hypothesis)\n",
    "\n",
    "    print(f\"Total de evaluaciones {i}, el costo final fue {loss}\")\n",
    "    w_opt=w\n",
    "    return w_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "# Parámetros de descenso de gradiente\n",
    "N_iterations = [50,60,80]\n",
    "\n",
    "# Pesos iniciales y ritmo de aprendizaje\n",
    "lr = 0.1  # Utiliza valores cercanos e.g. 0.05, 0.2 .. etc\n",
    "w_start = np.array([-400,0])\n",
    "w_start = np.expand_dims(w_start,-1)\n",
    "\n",
    "# (2.) - (4.) -> Variación de cantidad de iteraciones\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(x_val[:,-1], y_val)\n",
    "for n_it in N_iterations:\n",
    "    # Entrenamos el modelo en el set de entrenamiento\n",
    "    w_opt = gradient_descent(x_train, y_train, lr, n_it, w_start)\n",
    "\n",
    "    # Visualizamos la predicción en el set de validación\n",
    "    prediction = x_val @ w_opt\n",
    "    plt.plot(x_val[:,-1], prediction, label = str(n_it)+\" Iteraciones de GD\", linewidth=3)\n",
    "\n",
    "# Obtener la solución analítica\n",
    "analitic_sol=linear_model.LinearRegression()\n",
    "analitic_sol.fit(x_train, y_train)\n",
    "\n",
    "# Visualizar solución analitica\n",
    "plt.plot(x_val[:,-1], analitic_sol.predict(x_val),label = 'Solución analítica', linewidth=1, linestyle=\"dashed\", color=\"black\")\n",
    "# plt.ylim(bottom=-10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>TODO: Contesta las siguientes preguntas</b> \n",
    "- ¿Cuántas evaluaciones de la función de costo se evaluaron para encontrar la solución con un $lr=0.1$? ¿Cómo se compara este número a la solución por busqueda exhaustiva? \n",
    "- ¿Qué pasa si cambiamos la tasa de aprendizaje (lr) a un valor dos veces grande?\n",
    "- ¿Qué pasa si cambiamos la tase de aprendizaje a un valor dos veces más chico?\n",
    "- ¿Qué pasa si cambiamos la tasa de aprendizaje (lr) por un valor muy grande?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
